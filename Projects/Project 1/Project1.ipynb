{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22ec94e2-5abc-4c04-b5bb-dea32df2eb50",
   "metadata": {
    "id": "22ec94e2-5abc-4c04-b5bb-dea32df2eb50"
   },
   "source": [
    "# Implied-Vol Surface Completion (FC-NN + BS layer)\n",
    "## a lotta words to say that this project IS NOT COMPLETED YET!!!!\n",
    "\n",
    "\n",
    "**Goal (snapshot t_0):** Given a few option quotes at one timestamp, complete an **arbitrage-free** surface of prices/IV across strikes K and tenors T.  \n",
    "**Method:** Fully-connected MLP → predicted IV 𝜎̂(K,T) → **Black–Scholes** price layer → fit to observed quotes.  \n",
    "**Stretch:** Small **PINN** term (BS-PDE residual + simple boundary/terminal penalties) as regularizer.  \n",
    "**Deliverables:** error/arb metrics, plots, ablation (baseline / +no-arb / +PINN / ensemble).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b60a509-2fab-444b-bdcb-92e7cf230b81",
   "metadata": {
    "id": "0b60a509-2fab-444b-bdcb-92e7cf230b81"
   },
   "source": [
    "We will be building a black scholes model calculator in house and following this methodology:\n",
    " - Generate data using b-s model for pretraining\n",
    " - Utilize data from kaggle dataset(s) for fine-tuning\n",
    "\n",
    "\n",
    "maybe we can experiment with join curriculum, with 90% synthetic / 10% real and validate/test on real only\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289d5ec1-010c-4aba-887e-446cdc5e34b5",
   "metadata": {
    "id": "289d5ec1-010c-4aba-887e-446cdc5e34b5"
   },
   "source": [
    "## Data generation using the Black-Scholes model\n",
    "Use b-s to generate \"snapshots\" of many quotes. Within each snapshot, pick 10-20 quotes as \"observed\", while treating the rest as targets for completion. This is fixed only once for fair comparison. We then use this phase to debug the model/tune rough ranges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d06239-f926-4442-8174-d9231e5653da",
   "metadata": {
    "id": "c2d06239-f926-4442-8174-d9231e5653da"
   },
   "source": [
    "Black Scholes Formula (Call):\n",
    "$$ C = SN(d_1)-Ke^{rT}N(d_2) $$\n",
    "Where:\n",
    "- $C$: Price of the European call option\n",
    "- $S$: Current price of the underlying asset (spot)\n",
    "- $K$: Strike price of the option\n",
    "- $r$: Risk-free interest rate\n",
    "- $T$: Time to expiration (in years)\n",
    "- $\\sigma$: Volatility of the underlying asset's returns\n",
    "- $N(d_{1})$ and $N(d_{2})$: The cumulative standard normal distribution function, which gives the probability that a variable will be less than a certain value\n",
    "\n",
    "Put is different formula (to work on later)\n",
    "\n",
    "This function will need to be useful in data generation. It will be rebuilt inside the model itself for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "Il8LRmgrLEwh",
   "metadata": {
    "id": "Il8LRmgrLEwh"
   },
   "outputs": [],
   "source": [
    "# Global Imports\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import norm\n",
    "import math\n",
    "import platform\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8002d91-7c20-491a-842e-1f2869129f22",
   "metadata": {
    "id": "b8002d91-7c20-491a-842e-1f2869129f22"
   },
   "outputs": [],
   "source": [
    "def bs_price(spot, strike, years, r, q, sigma, option=\"call\"):\n",
    "    T = np.asarray(years, dtype=float)\n",
    "    S = np.asarray(spot, dtype=float)\n",
    "    K = np.asarray(strike, dtype=float)\n",
    "    sig = np.asarray(sigma, dtype=float)\n",
    "\n",
    "    eps = 1e-12  # fix divide by zero error\n",
    "\n",
    "    T = np.maximum(T, eps)\n",
    "    sig = np.maximum(sig, 1e-12)\n",
    "\n",
    "    d1 = (np.log(S / K) + (r - q + 0.5 * sig**2) * T) / (sig * np.sqrt(T))\n",
    "    d2 = d1 - sig * np.sqrt(T)\n",
    "\n",
    "    Nd1 = norm.cdf(d1)\n",
    "    Nd2 = norm.cdf(d2)\n",
    "\n",
    "    if option == \"call\":\n",
    "        return S * np.exp(-q * T) * Nd1 - K * np.exp(-r * T) * Nd2\n",
    "    else:  # put\n",
    "        return K * np.exp(-r * T) * norm.cdf(-d2) - S * np.exp(-q * T) * norm.cdf(-d1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e528e171-1703-4d79-9405-dcd291dc57d4",
   "metadata": {
    "id": "e528e171-1703-4d79-9405-dcd291dc57d4"
   },
   "outputs": [],
   "source": [
    "def make_snapshot(snapshot_id, n_strikes=30, tenors_days=(7,14,30,60,90,180,365),\n",
    "                  smile=False, rng=None):\n",
    "    rng = np.random.default_rng(rng)\n",
    "\n",
    "    S0 = rng.uniform(50, 500)\n",
    "    r = rng.uniform(0.00, 0.05)\n",
    "    q = rng.uniform(0.00, 0.03)\n",
    "\n",
    "    m = rng.uniform(0.6, 1.4, size=n_strikes)\n",
    "    K = np.sort(S0 * m)\n",
    "    T = np.array(tenors_days) / 365.0\n",
    "\n",
    "    K_grid, T_grid = np.meshgrid(K, T, indexing=\"xy\")\n",
    "    S_grid = np.full_like(K_grid, S0)\n",
    "    r_grid = np.full_like(K_grid, r)\n",
    "    q_grid = np.full_like(K_grid, q)\n",
    "\n",
    "    # Volatility: constant or simple smile\n",
    "    if not smile:\n",
    "        sigma_snap = rng.uniform(0.10, 0.60)\n",
    "        sigma_grid = np.full_like(K_grid, sigma_snap)\n",
    "    else:\n",
    "        ell = np.log(S0 / K_grid)\n",
    "        a = rng.uniform(0.10, 0.50)\n",
    "        b = rng.uniform(-0.20, 0.20)\n",
    "        c = rng.uniform(0.00, 0.20)\n",
    "        d = rng.uniform(-0.05, 0.10)\n",
    "        sigma_grid = np.clip(a + b*ell + c*ell**2 + d*np.sqrt(T_grid), 0.05, 2.0)\n",
    "\n",
    "    P_mid = bs_price(S_grid, K_grid, T_grid, r_grid, q_grid, sigma_grid, option=\"call\")\n",
    "\n",
    "    noise = rng.normal(loc=0.0, scale=0.01*np.maximum(0.1, P_mid))\n",
    "    P_obs = np.clip(P_mid + noise, 0.0, None)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"snapshot_id\": snapshot_id,\n",
    "        \"S0\": S_grid.ravel(),\n",
    "        \"K\": K_grid.ravel(),\n",
    "        \"T\": T_grid.ravel(),\n",
    "        \"r\": r_grid.ravel(),\n",
    "        \"q\": q_grid.ravel(),\n",
    "        \"sigma_true\": sigma_grid.ravel(),\n",
    "        \"price_mid\": P_mid.ravel(),\n",
    "        \"price_obs\": P_obs.ravel(),\n",
    "    })\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e71e6790-c821-46d7-bdfe-0fa194e26738",
   "metadata": {
    "id": "e71e6790-c821-46d7-bdfe-0fa194e26738"
   },
   "outputs": [],
   "source": [
    "def make_dataset(n_snapshots=2000, smile=False, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    dfs = []\n",
    "    for sid in range(n_snapshots):\n",
    "        df = make_snapshot(sid, smile=smile, rng=rng)\n",
    "        dfs.append(df)\n",
    "    return pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67d933ab-f559-45e2-a460-3a20c01bdd8d",
   "metadata": {
    "id": "67d933ab-f559-45e2-a460-3a20c01bdd8d"
   },
   "outputs": [],
   "source": [
    "def split_snapshots(n_snapshots, train=0.8, val=0.1, seed=7):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    ids = np.arange(n_snapshots)\n",
    "    rng.shuffle(ids)\n",
    "    n_train = int(train*n_snapshots)\n",
    "    n_val = int(val*n_snapshots)\n",
    "    return {\n",
    "        \"train\": ids[:n_train],\n",
    "        \"val\": ids[n_train:n_train+n_val],\n",
    "        \"test\":  ids[n_train+n_val:]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af4365eb-41c6-4ffe-8324-9b8f515b26b0",
   "metadata": {
    "id": "af4365eb-41c6-4ffe-8324-9b8f515b26b0"
   },
   "outputs": [],
   "source": [
    "def make_observed_mask(df, observed_per_snapshot=15, seed=99):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    mask = {}\n",
    "    for sid, df_s in df.groupby(\"snapshot_id\"):\n",
    "        idx = df_s.index.values\n",
    "        choose = min(observed_per_snapshot, len(idx))\n",
    "        obs_idx = rng.choice(idx, size=choose, replace=False)\n",
    "        mask[int(sid)] = np.sort(obs_idx)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6JJxfZ6klHqr",
   "metadata": {
    "id": "6JJxfZ6klHqr"
   },
   "source": [
    "# Model Creation\n",
    "Now that we have created the B-S Model and functions that will help us generate synthetic code, we can now work on the creation of the model itself. The architecture is as follows:\n",
    "- Fully connected Multi-Layer Perceptron (MLP)\n",
    "- Depth $\\times$ width: 5 hidden layers $\\times$ 256 units\n",
    "- Activation: SiLU (Sigmoid Linear Unit)\n",
    "\n",
    "The model has the following inputs:\n",
    "- log-moneyness: $ln(S0/K)$\n",
    "\t- how far the strike is from the spot\n",
    "- time to expiry: $\\tau$ in years\n",
    "- rates: risk-free $r$ and dividend yield $q$\n",
    "- _(optional)_ scaled spot $S_{0} / 100$\n",
    "\n",
    "The model would then output:\n",
    "- Implied Volatility $\\hat{\\sigma}(K,T)>0$\n",
    "  - We then plug $\\hat{\\sigma}$ into Black-Scholes to get a price $\\hat{C}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72157f20-a67e-425c-8fa9-a3a9712b3b3d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "72157f20-a67e-425c-8fa9-a3a9712b3b3d",
    "outputId": "76e87962-b4bf-4959-8b6e-6c57dfc2fd8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Found 1 CUDA device(s):\n",
      "  [0] NVIDIA GeForce RTX 4070 SUPER\n",
      "\n",
      "Python: 3.12.3 | PyTorch: 2.4.1\n"
     ]
    }
   ],
   "source": [
    "# Dr white's implementation\n",
    "# Device selection: CUDA -> MPS (Apple Silicon) -> CPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device: {device.type}\\n\")\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"Found {num_gpus} CUDA device(s):\")\n",
    "    for idx in range(num_gpus):\n",
    "        print(f\"  [{idx}] {torch.cuda.get_device_name(idx)}\")\n",
    "elif device.type == \"mps\":\n",
    "    print(\"Apple Metal (MPS) device available\")\n",
    "else:\n",
    "    print(\"Running on CPU\")\n",
    "\n",
    "print(\"\\nPython:\", platform.python_version(), \"| PyTorch:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "XtAXn8TplHKU",
   "metadata": {
    "id": "XtAXn8TplHKU"
   },
   "outputs": [],
   "source": [
    "class MLP_IV(nn.Module):\n",
    "    def __init__(self, in_dim=5, width=256, depth=5, eps=1e-4):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        dims = [in_dim] + [width]*depth\n",
    "        self.backbone = nn.Sequential(\n",
    "          nn.Linear(in_dim, width), nn.SiLU(),\n",
    "          nn.Linear(width, width), nn.SiLU(),\n",
    "          nn.Linear(width, width), nn.SiLU(),\n",
    "          nn.Linear(width, width), nn.SiLU(),\n",
    "          nn.Linear(width, width), nn.SiLU(),\n",
    "        )\n",
    "        self.head = nn.Linear(width, 1)\n",
    "        self.softplus = nn.Softplus()\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.backbone(x)\n",
    "        sigma = self.softplus(self.head(h)) + self.eps\n",
    "        return sigma.squeeze(-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0rlV8Qth2RNY",
   "metadata": {
    "id": "0rlV8Qth2RNY"
   },
   "source": [
    "Reasoning for using only the Call price.\n",
    "\"But wait! Why not make a `bs_put_price` too?\"\n",
    " - Answer: *They're not independent.* In fact, one can always be computed from the other using the **put-call parity** relationship:\n",
    "\n",
    " $$\n",
    "C - P = S_{0}e^{qT} - Ke^{-rT}\n",
    "$$\n",
    "or rearranged for the put:\n",
    "$$\n",
    "P = C - S_{0}e^{qT} + Ke^{-rt}.\n",
    "$$\n",
    "That means, once our network learns the **call surface** (call prices for all strikes/maturities), we can immediately compute the corresponding put surface **analytically.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "pEE7kZc-td8D",
   "metadata": {
    "id": "pEE7kZc-td8D"
   },
   "outputs": [],
   "source": [
    "def normal_cdf(x):\n",
    "    # Stable CDF via PyTorch's error function (keeps it differentiable)\n",
    "    # Used in calculating the call price in Black-Scholes\n",
    "    return 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n",
    "\n",
    "def bs_call_price(S, K, tau, r, q, sigma):\n",
    "    # Implementation of Black-Scholes Call price in PyTorch\n",
    "    S = torch.clamp(S, min=1e-12)\n",
    "    K = torch.clamp(K, min=1e-12)\n",
    "    tau = torch.clamp(tau, min=1e-12)\n",
    "    sqrt_tau = torch.sqrt(tau)\n",
    "\n",
    "    d1 = (torch.log(S/K) + (r-q + 0.5 * sigma**2) * tau) / (sigma * sqrt_tau)\n",
    "    d2 = d1 - sigma * sqrt_tau\n",
    "    Nd1 = normal_cdf(d1)\n",
    "    Nd2 = normal_cdf(d2)\n",
    "    disc_q = torch.exp(-q * tau)\n",
    "    disc_r = torch.exp(-r * tau)\n",
    "    C = S * disc_q * Nd1 - K * disc_r * Nd2\n",
    "    return C\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9gLPuR9IJ9KF",
   "metadata": {
    "id": "9gLPuR9IJ9KF"
   },
   "source": [
    "To ensure readability and ease-of-use for the results, we will be using Dataloaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "Iq9wQnQpJ5Kx",
   "metadata": {
    "id": "Iq9wQnQpJ5Kx"
   },
   "outputs": [],
   "source": [
    "# Custom dataset for the MLP Model\n",
    "class OptionDataset(Dataset):\n",
    "    def __init__(self, df, stats=None, standardize=True):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        # features for the MLP\n",
    "        log_mny = np.log(self.df[\"S0\"].values / self.df[\"K\"].values)\n",
    "        T = self.df[\"T\"].values\n",
    "        r = self.df[\"r\"].values\n",
    "        q = self.df[\"q\"].values\n",
    "        S0_scaled = self.df[\"S0\"].values / 100.0\n",
    "\n",
    "        # fit stats on train; reuse them for val/test\n",
    "        if stats is None:\n",
    "            stats = {\n",
    "              \"T_mu\": T.mean(), \"T_sd\": T.std() + 1e-8,\n",
    "              \"r_mu\": r.mean(), \"r_sd\": r.std() + 1e-8,\n",
    "              \"q_mu\": q.mean(), \"q_sd\": q.std() + 1e-8,\n",
    "            }\n",
    "        self.stats = stats\n",
    "\n",
    "        if standardize:\n",
    "            Tn = (T - stats[\"T_mu\"]) / stats[\"T_sd\"]\n",
    "            rn = (r - stats[\"r_mu\"]) / stats[\"r_sd\"]\n",
    "            qn = (q - stats[\"q_mu\"]) / stats[\"q_sd\"]\n",
    "        else:\n",
    "            Tn, rn, qn = T, r, q\n",
    "\n",
    "        X = np.stack([log_mny, Tn, rn, qn, S0_scaled], axis=1).astype(np.float32)\n",
    "\n",
    "        # store tensors\n",
    "        self.X = torch.from_numpy(X)\n",
    "        self.S0 = torch.from_numpy(self.df[\"S0\"].values.astype(np.float32))\n",
    "        self.K = torch.from_numpy(self.df[\"K\"].values.astype(np.float32))\n",
    "        self.T = torch.from_numpy(self.df[\"T\"].values.astype(np.float32))   # raw for BS\n",
    "        self.r = torch.from_numpy(self.df[\"r\"].values.astype(np.float32))\n",
    "        self.q = torch.from_numpy(self.df[\"q\"].values.astype(np.float32))\n",
    "        self.P = torch.from_numpy(self.df[\"price_obs\"].values.astype(np.float32))\n",
    "\n",
    "    def __len__(self): return len(self.df)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return {\n",
    "            \"features\": self.X[i],\n",
    "            \"S0\": self.S0[i], \"K\": self.K[i], \"T\": self.T[i],\n",
    "            \"r\": self.r[i], \"q\": self.q[i],\n",
    "            \"P_obs\": self.P[i],\n",
    "            \"stats\": self.stats,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b_CEEhRE4clx",
   "metadata": {
    "id": "b_CEEhRE4clx"
   },
   "source": [
    "## Creating the Loss function\n",
    "Due to the nature of the problem we are trying to solve, it would be naive to implement a single function to act as our Loss. Intuitively, we can approach this with a more modular mindset.\n",
    "Instead of implementing one large Loss function $L$, we create three separate parts of a greater whole. The benefits of doing this include:\n",
    " - Debugging/monitoring each piece separately\n",
    " - Turning on/off certain loss functions during training\n",
    " - Allowing certain loss functions to be weighted differently\n",
    "\n",
    " If we just use a single formula, we can't tell whether the model is missing because it's not fitting the data or because a regularizer is too strong.\n",
    "\n",
    " We can proceed with the mental model:\n",
    "1. A fit-to-market loss function `loss_fit_price`\n",
    "\t- It uses only the quotes observed (our 15 per snapshot)\n",
    "\t- It teaches the model to \"Match the market at the points we actually saw.\"\n",
    "\t- This is the \"Primary teacher\"\n",
    "2. No-arbitrage shape penalty `loss_noarb_bounds`\n",
    "\t- Uses the model's predicted prices at supervised points\n",
    "\t- It teaches the model to \"stay inside basic bounds!\"\n",
    "\t\t- $0\\leq C \\leq S_0 e^{-qT}$\n",
    "\t- These are soft constraints/regularizers, small weights. We don't want them to dominate.\n",
    "3. Physics (PDE) Residual + Boundary/Terminal `loss_pde_and_bc`\n",
    "\t- It uses collocation points we label, autodiff to get $u_t,u_S,u_{SS}$ and check the Black-Scholes PDE and simple boundary/terminal conditions.\n",
    "\t- It teaches the model to \"Be locally consistent with the BS dynamics, respecting the payoff at certain points\".\n",
    "\t- Since it doesn't use labeled data, it should be a different kind of term\n",
    "\t- **Idea:** Start training without it **(warm-up)** and then turn it on with a small weight, so it regularizes rather than overpowers the fit.\n",
    "\n",
    "\n",
    "Overall, the total loss is:\n",
    "\n",
    "$$\n",
    "L = L_{\\text{fit}} + \\lambda_{\\text{arb}} \\space L_{\\text{arb}} + \\lambda_{\\text{pde}} \\space L_{\\text{pde}} + \\lambda_{\\text{bc}} \\space L_{\\text{bc}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "L_{\\text{arb}} = \\mathbb{E}(\\text{ReLU}(0-C)^2 + \\text{ReLU}(C-U)^2)\n",
    "$$\n",
    "\n",
    "For the PDE Loss function, just the PDE residual alone is not enough. Many surfaces can satisfy it locally. Therefore, we need three panlties to make the solution unique/physically sensible across the whole domain. These are:\n",
    "1. Terminal Payoff $(\\tau \\approx 0)$\n",
    "\t- \"Initial Condition\" in time\n",
    "    - Enforces the idea that the option is worth its payoff at expiry.\n",
    "    - Without it, many different surfaces could satisfy the PDE locally, but disagree at expiry.\n",
    "2. Zero-spot boundary (S = 0)\n",
    "\t- Left boundary in price space\n",
    "    - Enforces \"uselessness\" to a call for any $\\tau > 0$.\n",
    "    - Prevents the network from inventing positive values when the call itself is worthless.\n",
    "    - S -> 0\n",
    "3. Large-S slope (Delta)\n",
    "\t- Right boundary in price space\n",
    "    - Enforces the idea that, for large S, a call behaves like stock with divident yield q: $\\frac{\\partial u}{\\partial S} \\approx e^{-qT}$\n",
    "    - Delta is required to prevent the prices to explode/dominate the loss when S is high.\n",
    "    - S -> $\\infty$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "Bf7sVa2_ti3I",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "id": "Bf7sVa2_ti3I",
    "outputId": "2a2cefa5-ae59-470c-b1a4-86abebe748e1"
   },
   "outputs": [],
   "source": [
    "# Compute the supervised fit loss\n",
    "def loss_fit_price(model, batch):\n",
    "\n",
    "    # Pull the tensors from the batch\n",
    "    X = batch[\"features\"]\n",
    "    S0 = batch[\"S0\"]\n",
    "    K = batch[\"K\"]\n",
    "    T = batch[\"T\"]\n",
    "    r = batch[\"r\"]\n",
    "    q = batch[\"q\"]\n",
    "    P = batch[\"P_obs\"] # Observed call prices\n",
    "\n",
    "    # Predict Implied Volatility based on the tensors\n",
    "    sigma_hat = model(X)\n",
    "\n",
    "    # Decode to price with the Black-Scholes model\n",
    "    C_hat = bs_call_price(S0, K, T, r, q, sigma_hat)\n",
    "\n",
    "    # Compute the fit loss (MAE)\n",
    "    L_fit = F.l1_loss(C_hat,P)\n",
    "\n",
    "    # Return BOTH the scalar loss and a small cache to reuse later\n",
    "    return L_fit, {\"sigma_hat\": sigma_hat, \"C_hat\": C_hat}\n",
    "\n",
    "# Penalize prices that violate the basic no-arbitrage bounds for European Calls\n",
    "def loss_noarb_bounds(C_hat, S0, T, q):\n",
    "    # Tiny epsilon to reduce noise\n",
    "    epsilon = 1e-8\n",
    "    \n",
    "    # Compute the upper bound for every contract\n",
    "    U = S0 * torch.exp(-q * T)\n",
    "\n",
    "    # Measure violations (Upper/Lower), ReLU keeps penalty 0 when inside bounds\n",
    "    lower_bound_violation = torch.relu((0.0 - epsilon) - C_hat)\n",
    "    upper_bound_violation = torch.relu(C_hat - (U + epsilon))\n",
    "\n",
    "    # Return a penalty scalar based on the bound violations\n",
    "    return (upper_bound_violation.pow(2) + lower_bound_violation.pow(2)).mean()\n",
    "\n",
    "# Create synthetic points where we don't have the market quotes\n",
    "def sample_collocation(batch, n_colloc=None, s_factor=(0.4, 2.5)):\n",
    "\n",
    "    N = batch[\"S0\"].shape[0]\n",
    "    if n_colloc is None:\n",
    "        n_colloc = N\n",
    "    device = batch[\"S0\"].device\n",
    "\n",
    "    # sample indices from the current batch\n",
    "    idx = torch.randint(0, N, (n_colloc,), device=device)\n",
    "\n",
    "    # gather base tensors\n",
    "    S0_ref = batch[\"S0\"][idx]\n",
    "    K = batch[\"K\"][idx]\n",
    "    r = batch[\"r\"][idx]\n",
    "    q = batch[\"q\"][idx]\n",
    "\n",
    "    # tau in [0, T_max]\n",
    "    T_max = torch.clamp(batch[\"T\"].max(), min=1e-8)\n",
    "    tau = torch.rand(n_colloc, device=device) * T_max\n",
    "\n",
    "    # S in [smin, smax] * S0_ref\n",
    "    smin, smax = s_factor\n",
    "    S = S0_ref * (smin + (smax - smin) * torch.rand(n_colloc, device=device))\n",
    "\n",
    "    # build model inputs X_coll with SAME normalization as training\n",
    "    stats = batch[\"stats\"]\n",
    "    T_mu = torch.tensor(stats[\"T_mu\"], device=device, dtype=torch.float32)\n",
    "    T_sd = torch.tensor(stats[\"T_sd\"], device=device, dtype=torch.float32)\n",
    "    r_mu = torch.tensor(stats[\"r_mu\"], device=device, dtype=torch.float32)\n",
    "    r_sd = torch.tensor(stats[\"r_sd\"], device=device, dtype=torch.float32)\n",
    "    q_mu = torch.tensor(stats[\"q_mu\"], device=device, dtype=torch.float32)\n",
    "    q_sd = torch.tensor(stats[\"q_sd\"], device=device, dtype=torch.float32)\n",
    "\n",
    "    log_mny = torch.log(S0_ref / K)\n",
    "    tau_n = (tau - T_mu) / T_sd\n",
    "    r_n = (r - r_mu) / r_sd\n",
    "    q_n = (q - q_mu) / q_sd\n",
    "    S0_scaled = S0_ref / 100.0\n",
    "\n",
    "    #Covert 5 separate 1-D tensors into one 2-D Tensor\n",
    "    X_coll = torch.stack([log_mny, tau_n, r_n, q_n, S0_scaled], dim=-1)\n",
    "\n",
    "    return {\n",
    "            \"S\": S, \"K\": K, \"tau\": tau, \"r\": r, \"q\": q,\n",
    "            \"X_coll\": X_coll, \"stats\": batch[\"stats\"],\n",
    "            \"S0_ref\": S0_ref\n",
    "           }\n",
    "    \n",
    "\n",
    "\n",
    "def loss_pde_and_bc(model, coll, lambda_bc=1e-4):\n",
    "\n",
    "    # Pull tensors from coll\n",
    "    X_coll = coll[\"X_coll\"]\n",
    "    S = coll[\"S\"].detach().clone().requires_grad_(True)\n",
    "    tau = coll[\"tau\"].detach().clone().requires_grad_(True)\n",
    "    K = coll[\"K\"]\n",
    "    r = coll[\"r\"]\n",
    "    q = coll[\"q\"]\n",
    "    stats = coll[\"stats\"]\n",
    "    S0_ref = coll[\"S0_ref\"]\n",
    "\n",
    "    # Predict Implied Volatility at collocation points\n",
    "    sigma_hat = model(X_coll)\n",
    "\n",
    "    #Price surface at collocation points\n",
    "    u = bs_call_price(S, K, tau, r, q, sigma_hat)\n",
    "\n",
    "    # Take first derivative of u w.r.t S\n",
    "    u_S = torch.autograd.grad(\n",
    "        outputs = u, \n",
    "        inputs = S, \n",
    "        grad_outputs = torch.ones_like(u),\n",
    "        create_graph = True\n",
    "    )[0]\n",
    "\n",
    "    # Take second derivative of u w.r.t S\n",
    "    u_SS = torch.autograd.grad(\n",
    "        outputs = u_S,\n",
    "        inputs = S,\n",
    "        grad_outputs = torch.ones_like(u_S),\n",
    "        create_graph = True\n",
    "    )[0]\n",
    "\n",
    "    # Take time derivative of u w.r.t tau\n",
    "    u_tau = torch.autograd.grad(\n",
    "        outputs = u,\n",
    "        inputs = tau,\n",
    "        grad_outputs = torch.ones_like(u),\n",
    "        create_graph = True\n",
    "    )[0]\n",
    "\n",
    "    # Form the Black-Scholes PDE residual\n",
    "    R_PDE = u_tau + 0.5 * sigma_hat**2 * S.pow(2) * u_SS + (r - q) * S * u_S - (r * u)\n",
    "\n",
    "    L_pde = torch.mean(R_PDE.pow(2))\n",
    "\n",
    "    # terminal payoff - Rebuild input features with a small tau value (tau0 = 1e-6) to get a new Implied Volatility\n",
    "    tau0 = torch.full_like(tau, 1e-6)\n",
    "    \n",
    "     # required: previous parameters from batch\n",
    "    T_mu = torch.tensor(stats[\"T_mu\"], device=device, dtype=torch.float32)\n",
    "    T_sd = torch.tensor(stats[\"T_sd\"], device=device, dtype=torch.float32)\n",
    "    r_mu = torch.tensor(stats[\"r_mu\"], device=device, dtype=torch.float32)\n",
    "    r_sd = torch.tensor(stats[\"r_sd\"], device=device, dtype=torch.float32)\n",
    "    q_mu = torch.tensor(stats[\"q_mu\"], device=device, dtype=torch.float32)\n",
    "    q_sd = torch.tensor(stats[\"q_sd\"], device=device, dtype=torch.float32)\n",
    "    \n",
    "    log_mny = torch.log(S0_ref / K)\n",
    "    tau_n = (tau0 - T_mu) / T_sd\n",
    "    r_n = (r - r_mu) / r_sd\n",
    "    q_n = (q - q_mu) / q_sd\n",
    "    S0_scaled = S0_ref / 100.0\n",
    "\n",
    "    X_tau0 = torch.stack([log_mny, tau_n, r_n, q_n, S0_scaled], dim=-1)\n",
    "\n",
    "    sigma_hat_tau0 = model(X_tau0)\n",
    "\n",
    "    u_tau0 = bs_call_price(S, K, tau0, r, q, sigma_hat_tau0)\n",
    "    payoff = torch.relu(S - K)\n",
    "    L_term = F.mse_loss(u_tau0, payoff)\n",
    "\n",
    "    # zero spot boundary (left boundary in price space)\n",
    "    u_S0 = bs_call_price(torch.zeros_like(S), K, tau, r, q, sigma_hat)\n",
    "    L_S0 = torch.mean(u_S0**2)\n",
    "\n",
    "    # Large-S delta behavior (right boundary in price space)\n",
    "    S_big = 2.5 * S0_ref\n",
    "    S_big = S_big.detach().clone().requires_grad_(True)\n",
    "    u_big = bs_call_price(S_big, K, tau, r, q, sigma_hat)\n",
    "    u_S_big = torch.autograd.grad(\n",
    "        outputs=u_big,\n",
    "        inputs=S_big,\n",
    "        grad_outputs=torch.ones_like(u_big),\n",
    "        create_graph=False\n",
    "    )[0]\n",
    "\n",
    "    delta = torch.exp(-q * tau)\n",
    "\n",
    "    L_delta = F.mse_loss(u_S_big, delta)\n",
    "\n",
    "    # Combine all three terms\n",
    "    L_bc = L_term + L_S0 + L_delta\n",
    "    \n",
    "    # return L_pde and L_bc * lambda_bc\n",
    "    return L_pde, L_bc * lambda_bc\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VWI7_a1S1i6d",
   "metadata": {
    "id": "VWI7_a1S1i6d"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "289d5ec1-010c-4aba-887e-446cdc5e34b5"
   ],
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (Jupyter)",
   "language": "python",
   "name": "jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
